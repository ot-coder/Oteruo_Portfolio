---
title: "UDACITY: Introducing Generative AI with AWS"
date: "2024-06-01"
excerpt: "My journey partaking in this course on AI with AWS."
coverImage: "/placeholder.svg?height=500&width=1000"
---

This is a very fascinating course I have recently completed provided by Udacity on Generative AI. It covered interesting topics about AI, beginning with fundamentals of AI, expanding into the use of LLMs (Large Language Models), the science behind this, which, only 5 years ago, was believed to be a magical intelligent application of AI. The course then went on to educate me(and other participants) on real-world applications of AI, sparking my imagination about the potential uses of Generative AI in various fields.

As a computer science student, this is a particularly intriguing technology which I am excited to delve deeper into. Having previously completed an introductory course taught by Stanford University, I gained a different perspective on AI, particularly LLMs, which are all the rage in 2025. I gained more hands-on experience using Jupyter Notebook, which allowed me to deepen my Python skills. Additionally, I utilized Amazon services to create a basic PartyRock app. 

Fundamentals of AI and ML

This section was an overview of Machine Learning, introducing the two basic types(Supervised and Unsupervised Learning). Supervised learning resembles what is practised in real-life educational institutions. In which a teacher (you, as the programmer) provides data to the student (the algorithm), and the student learns what is correct by seeing already labelled pairs. To understand this, recall how we are taught colours in kindergarten. You are given different objects of a similar colour and told what colour these are, eg a red apple, a stop sign, a red shirt, etc. Afterwards, you are tested by showing different objects and asked to find the red one. This is the exact same concept, no matter the complexity. 

Let's make it more relatable to real life and deal with a simple example, predicting house prices. To determine how much you would sell a house, you require the location, size, possible features and other details. In supervised learning, you would obtain data about various houses sold, and note these details(known in ML as Features) and map these features to said prices(known in ML as Labels). This gives the algorithm the ability to find correlations and learn what possible combination of features would cause a price. Next, ensure your algorithm is functioning as expected (giving accurate predictions) before deployment. In order to do that, you would then make use of your validation set to tweak the settings. Think of this like adjusting the picture or audio settings on a TV to your liking. Using the validation set, you would perform tasks such as **regularisation (a process of instructing your model not to rely too heavily on specific features by penalising overreliance), feature selection, etc. Finally, you test your model on the testing set. Note that the goal is not to get the perfect price on each house in the testing set, as this is classified as overfitting. Why not? Surely, you want to make sure you have the highest accuracy rate. Logically, this would make sense; however, it can also mean that your model has simply memorized your data and thus would not be very useful on unfamiliar datasets or predictions. The aim is rather to have a reasonably accurate prediction of all the house prices (Labels).

Now, what if you had no labels at all — no prices, no tags, no correct answers — just raw, unstructured data? That's where unsupervised learning comes in. Unlike supervised learning, the algorithm isn't told what to look for; instead, it tries to find patterns and relationships entirely on its own. Think of it like trying to organize a box of puzzle pieces without the final picture — you rely on similarities in shape, colour, or texture to make sense of the chaos. This approach is beneficial when you don't know exactly what you're looking for but still want the model to reveal some hidden structure.

A key concept here is clustering, which involves grouping data points that are similar to one another based on their features. You've probably seen this in action without realizing it — for example, when your phone automatically organizes your photo gallery by faces, or when a streaming app recommends songs that "just feel right." No one told the algorithm what was in those photos or what genre a song belongs to — it discovered those groupings on its own. In the real world, clustering is often used for customer segmentation, anomaly detection, or even organizing news articles by theme. It's one of those techniques that makes you step back and think: the model didn't need instructions — it just required data.
